#!/usr/bin/env python
""" commandline tool to grab parse article text and metadata from a given url """

import urllib2
import logging
from datetime import datetime,timedelta
from optparse import OptionParser
import csv

try:
    import simplejson as json
except ImportError:
    import json

import metareadability
import decruft

import site
site.addsitedir("../pylib")
from JL import ScraperUtils
from JL import ukmedia



def context_from_url(url):
    context = {}
    context['permalink']=url
    context['srcurl']=url
    context['lastseen'] = datetime.now()
    return context

def extract(html,context,**kwargs):
    art = context

    kw = {}
    if 'encoding' in kwargs:
        kw['encoding'] = kwargs['encoding']

    headline,byline,pubdate = metareadability.extract(html,context['srcurl'], **kw)
    if headline is not None:
        art['title'] = headline
    if pubdate is not None:
        art['pubdate'] = pubdate
    if byline is not None:
        art['byline'] = byline
    else:
        art['byline'] = u''

    txt = decruft.Document(html).summary()
    art['content'] = ukmedia.SanitiseHTML(txt)
    return art


def main():
    max_errors = 100

    parser = OptionParser(usage="%prog: [options] urls")
    parser.add_option('-v', '--verbose', action='store_true')
    parser.add_option('-d', '--debug', action='store_true')
    parser.add_option('-t', '--test', action='store_true', help="test only - don't commit to db")
    parser.add_option('-f', '--force_rescrape', action='store_true')
    parser.add_option('-s', '--source_feeds', dest="source_feeds", help="list of rss feeds, file in csv format")
    parser.add_option('-j', '--source_feeds_json', action='store_true', help="rss feeds, in json format, not csv")
    parser.add_option("-m", "--max_errors", type="int", default=max_errors, help="set num of errors allowed before quitting (default %d)" % (max_errors,))
    parser.add_option("-a", "--max_age", type="int", default=None, help="discard any articles older than x hours old (only if feeds -s is used)")
    (options, args) = parser.parse_args()

    log_level = logging.ERROR
    if options.debug:
        log_level = logging.DEBUG
    if options.verbose:
        log_level = logging.INFO

    logging.basicConfig(level=log_level, format='%(message)s')

    if options.source_feeds:
        feedsfile = open(options.source_feeds,"rt")
        if options.source_feeds_json:
            feeds = json.loads(feedsfile.read())
            feeds = [(f[1],f[0]) for f in feeds]
        else:
            # csv format
            reader = csv.reader(feedsfile)
            feeds = [f for f in reader]

        print "using feeds from ", options.source_feeds

        arts = ScraperUtils.FindArticlesFromRSS(feeds, None, None, maxerrors=20 )

        if options.max_age is not None:
            cutoff = datetime.now() - timedelta(hours=options.max_age)

            logging.info("discarding anything before %s" % (cutoff))
            logging.info("before cutoff: %d" %(len(arts),))
            arts = [a for a in arts if a.get('pubdate', datetime.now()) > cutoff]
            logging.info("after cutoff: %d" %(len(arts),))


        ScraperUtils.scrape_articles(arts, extract, options)


    else:
        # individual urls
        arts = [context_from_url(url) for url in args]

        ScraperUtils.scrape_articles(arts, extract, options)

if __name__ == '__main__':
    main()

