#!/usr/bin/env python
""" crawl a site for rss feeds """

import urllib2
import urlparse
import logging
from datetime import datetime
from optparse import OptionParser
import lxml.html
try:
    import simplejson as json
except ImportError:
    import json


import site
site.addsitedir("../pylib")
from JL import ScraperUtils
from JL import ukmedia




def crawl(root_url, max_depth=2):
    """ crawl a site for rss feeds """

    err_cnt = 0
    max_errs = 10

    o = urlparse.urlparse(root_url)
    root_host = o[1]

    feeds = {}
    visited = set()
    queued = set()
    queued.add( (root_url,0) )

    while queued:
        (page_url,depth) = queued.pop()
        visited.add(page_url)
        try:
            logging.debug("fetch %s (depth %d)" % (page_url,depth))
            resp = urllib2.urlopen(page_url)
            html = resp.read()
            # add any URLs we were redirected via...
#            for code,url in resp.redirects:
#                known_urls.add(url)
#                if code==301:    # permanant redirect
#                    context['permalink'] = url
        except urllib2.HTTPError, e:
            err_cnt += 1
            if err_cnt >= max_errs:
                logging.critical('error count exceeded - BAILING')
                raise
            logging.error('%s: %s\n' % (page_url,str(e)))
            continue

        doc = lxml.html.fromstring(html)
        doc.make_links_absolute(page_url)

        # any rss feeds on this page?
        feed_types = ('application/rss+xml','application/atom+xml','text/xml')
        for alt in doc.cssselect('head link[rel="alternate"]'):
            type = alt.get('type','')
            title = alt.get('title', '')
            href = alt.get('href','')

            if title.lower().endswith('comments feed'):
                logging.info("skip comments feed %s" % (href,))
                continue

            if type in feed_types:
                if href not in feeds:
                    logging.info("found %s - %s (%s)" %(href,title,type))
                    feeds[href] = (href,title,type)

        # now look for other links to follow from this page
        for a in doc.cssselect('a'):
            url = a.get('href')
            if url is None:
                continue

            # kill query and fragment parts
            o = urlparse.urlparse(url)
            url = urlparse.urlunparse((o[0], o[1], o[2], o[3], '', ''))

            if url not in visited and depth<max_depth:
                # try and stay on same site (subdomains ok)...
                if o[1].endswith(root_host):
                    #logging.debug("queue %s (depth %d)" %(url,depth+1))
                    queued.add((url,depth+1))

        #print("scan %s (%d articles)\n" % (page_url,art_cnt))
    logging.info("scanned %d pages, found %d feeds" %(len(visited),len(feeds)))

    return feeds.values()



def main():
    parser = OptionParser(usage="%prog: [options] urls")
    parser.add_option('-j', '--json', help="output feeds as json", action='store_true')
    parser.add_option('-v', '--verbose', action='store_true')
    parser.add_option('-V', '--debug', action='store_true')
    parser.add_option('-m', '--max_depth', type="int", default=1, dest="max_depth")
    (options, args) = parser.parse_args()

    log_level = logging.ERROR
    if options.verbose:
        log_level = logging.INFO
    if options.debug:
        log_level = logging.DEBUG
    logging.basicConfig(level=log_level, format='%(message)s')

    feeds = crawl(args[0],int(options.max_depth))
    if options.json:
        print json.dumps(feeds)
    else:
        for f in feeds:
            print '%s "%s" (%s)' % (f[0],f[1],f[2])


if __name__ == '__main__':
    main()

