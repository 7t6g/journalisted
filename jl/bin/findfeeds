#!/usr/bin/env python
""" crawl a site for rss feeds """

import urllib2
import httplib # for HTTPExeption
import urlparse
import logging
import re
import csv
import sys
from datetime import datetime
from optparse import OptionParser
import lxml.html


def crawl(root_url, max_depth=2):
    """ crawl a site for rss feeds """

    err_cnt = 0
    max_errs = 10

    o = urlparse.urlparse(root_url)
    root_host = o[1]

    feeds = {}
    visited = set()
    queued = set()
    queued.add( (root_url,0) )

    while queued:
        (page_url,depth) = queued.pop()
        visited.add(page_url)
        try:
            logging.debug("fetch %s (depth %d)" % (page_url,depth))
            resp = urllib2.urlopen(page_url)
            html = resp.read()
            doc = lxml.html.fromstring(html)
            doc.make_links_absolute(page_url)

        except Exception as e:
            err_cnt += 1
            if err_cnt >= max_errs:
                logging.critical('error count exceeded - BAILING')
                raise
            logging.error('ERROR on %s: %s\n' % (page_url,str(e)))
            continue


        # any rss feeds on this page?
        feed_types = ('application/rss+xml','application/atom+xml','text/xml', 'application/rssxml' )   # rssxml found on arstechnica (bug?)

        for alt in doc.cssselect('head link[rel="alternate"]'):
            type = unicode(alt.get('type',''))
            title = unicode(alt.get('title', ''))
            href = unicode(alt.get('href',''))

            if re.compile(r'\bcomments\b',re.I).search(title):
                logging.info("skip comments feed %s" % (href,))
                continue

            if type in feed_types:
                if href not in feeds:
                    logging.info("found %s %s" %(title,href))
                    feeds[href] = (title,href)

        # now look for other links to follow from this page
        for a in doc.cssselect('a'):
            url = a.get('href')
            if url is None:
                continue

            # kill query and fragment parts
            o = urlparse.urlparse(url)
            url = urlparse.urlunparse((o[0], o[1], o[2], o[3], '', ''))

            if url not in visited and depth<max_depth:
                # try and stay on same site (subdomains ok)...
                if o[1].endswith(root_host):
                    #logging.debug("queue %s (depth %d)" %(url,depth+1))
                    queued.add((url,depth+1))

        #print("scan %s (%d articles)\n" % (page_url,art_cnt))
    logging.info("scanned %d pages, found %d feeds" %(len(visited),len(feeds)))

    return feeds.values()



def main():
    desc = """Tool to crawl website(s) looking for rss feeds.
Writes resultant list to stdout.
        """
    parser = OptionParser(usage="%prog: [options] url", description=desc)
    parser.add_option('-v', '--verbose', action='store_true')
    parser.add_option('-d', '--debug', action='store_true')
    parser.add_option('-m', '--max_depth', type="int", default=1, dest="max_depth")
    parser.add_option('-o', '--outfile', help="output filename (default stdout)")
    (options, args) = parser.parse_args()

    log_level = logging.WARNING
    if options.verbose:
        log_level = logging.INFO
    if options.debug:
        log_level = logging.DEBUG
    logging.basicConfig(level=log_level, format='%(message)s')

    if len(args)<1:
        parser.error("No url specified")
    if len(args)>1:
        parser.error("Only one url at a time, please")

    feeds = crawl(args[0],int(options.max_depth))

    out = sys.stdout
    if options.outfile:
        out = open(options.outfile,'wt')

    w = csv.writer(out)
    for f in feeds:
        g = [foo.encode('utf-8') for foo in f]
        w.writerow(g)

if __name__ == '__main__':
    main()

