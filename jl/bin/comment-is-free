#!/usr/bin/env python2.4
#
# Tool to scrape journo info from the guardian commentisfree site.
#

import re
import urllib2
import sys

sys.path.append( "../pylib" )
from BeautifulSoup import BeautifulSoup, SoupStrainer
from JL import ukmedia


contributors_url = 'http://commentisfree.guardian.co.uk/contributors_a-z.html'


def ParseContributors( html ):
	"""parse the CIF contributors a-z page, returns a list of profile pages found"""

	soup = BeautifulSoup( html )

	def soup_findAll(tag, attrs):  # iterative soup.findAll to speed debugging
		strainer = SoupStrainer(tag, attrs)
		for x in soup.recursiveChildGenerator():
			found = strainer.search(x)
			if found: yield found

	profiles = []
	for authordiv in soup_findAll('div', {'class':'authorazentry'}):
		name = authordiv.h1.a.string
		name = ukmedia.DescapeHTML( name )
		a = authordiv.find( text="Profile" ).parent;
		profile_url = a['href']
		feed_url = profile_url.replace('/profile.html', '/index.xml')
		biodiv = authordiv.find('div', {'class': 'authorbio'})
		bio = biodiv.p.renderContents(None)
#		print name, profile_url, feed_url, bio

		profiles.append({'name': name, 'profile_url': profile_url,
		                 'feed_url': feed_url, 'bio': bio})

	return profiles


def Output( profiles, fout ):
	encoding = 'utf-8'
	fout.write( "<?xml version=\"1.0\" encoding=\"%s\"?>\n" %(encoding) )
	fout.write( " <journodata>" )
	for p in profiles:
		fout.write( "  <journo>\n" )
		fout.write( "   <name>%s</name>\n" %( p['name'].encode(encoding) ) )
		fout.write( "   <bio>%s</bio>\n" %( p['bio'].encode(encoding) ) )
		fout.write( "   <weblink url=\"%s\">%s</weblink>\n" % ( p['profile_url'], 'Profile page on Comment Is Free' ) )
		fout.write( "   <feed url=\"%s\">%s</feed>\n" % ( p['feed_url'], 'RSS feed of articles on Comment Is Free' ) )
		fout.write( "  </journo>\n" )
	fout.write( " </journodata>\n" )
	fout.write( "</xml>\n" );


if __name__=='__main__':
	if '--help' in sys.argv or '-h' in sys.argv:
		sys.exit('usage: comment-is-free\n'
		         'Outputs an XML file containing the feed for each journalist.')
	f = urllib2.urlopen( contributors_url )
	html = f.read()
	f.close()
	profiles = ParseContributors(html)
	Output( profiles, sys.stdout )
