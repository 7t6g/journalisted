#!/usr/bin/env python2.4
#
# Tool to scrape journo info from the guardian commentisfree site.
#

import re
from datetime import datetime
import sys
import simplejson

sys.path.append( "../pylib" )
from BeautifulSoup import BeautifulSoup, SoupStrainer
from JL import ukmedia, ArticleDB, FindJournoException


#CONTRIBUTORS_URL = 'http://commentisfree.guardian.co.uk/contributors_a-z.html'
CONTRIBUTORS_URL = 'file:contributors_a-z.html'  # debug


def run():
    """Parse the CIF contributors a-z page, update journo_bio and journo_weblink."""

    html = ukmedia.FetchURL(CONTRIBUTORS_URL)
    soup = BeautifulSoup(html)

    db = ArticleDB.ArticleDB()
    cursor = db.conn.cursor()
    cursor.execute("SELECT id FROM organisation WHERE shortname='guardian'")
    guardian_srcorgid = cursor.fetchone()[0]
    cursor.close()
    
    # FIXME: remove limit=10 after testing:
    authordivs = soup.findAll('div', {'class':'authorazentry'}, limit=10)

    # Ensure journos exist
    for authordiv in authordivs:
        name = ukmedia.DescapeHTML(authordiv.h1.a.string)
        journo_id = ArticleDB.StoreJourno(db.conn, name, guardian_srcorgid)

    cursor = db.conn.cursor()
    cursor.execute('BEGIN')

    exceptions = 0
    MAX_EXCEPTIONS = 100
    for authordiv in authordivs:
        biodiv = authordiv.find('div', {'class': 'authorbio'})
        
        name = ukmedia.DescapeHTML(authordiv.h1.a.string)
        profile_url = authordiv.find( text="Profile" ).parent['href']
        bio = biodiv.p.renderContents(None)
        
        try:
            journo_id = ArticleDB.StoreJourno(db.conn, name, guardian_srcorgid)
            save_journo_info(cursor, name, journo_id, profile_url, bio)
        except FindJournoException, e:
            ukmedia.DBUG2(u"Failed to get journo_id for %s:\n    %s\n" % (name, e))
            exceptions += 1
            if exceptions > MAX_EXCEPTIONS:
                raise  # rollback

    cursor.execute('COMMIT')
    cursor.close()

def save_journo_info(cursor, name, journo_id, profile_url, bio):
    assert journo_id is not None
    blog_url = profile_url.replace('/profile.html', '/')
    # Not storing: feed_url = profile_url.replace('/profile.html', '/index.xml')

    context = {'journo_name': name,
               'added': datetime.now().isoformat()[:19],
               'scraper': '/bin/comment-is-free'}
    context = simplejson.dumps(context)
    
    # Insert bio with srcurl=profile_url into journo_bio table:
    cursor.execute("SELECT id FROM journo_bio WHERE type='cif:contributors-az' "
                   "AND journo_id=%d" % journo_id)
    if cursor.fetchall():
        ukmedia.DBUG2('  not updating bio for %s\n' % name)
    else:
        ukmedia.DBUG2('  NEW bio for %s\n' % name)
        cursor.execute(
            "INSERT INTO journo_bio"
                "(context, bio, journo_id, srcurl, type, approved) "
            "VALUES (%s, %s, %s, %s, 'cif:contributors-az', false)",
            [context, bio, journo_id, profile_url])

    # Insert link for blog_url into journo_weblink
    cursor.execute("SELECT id FROM journo_weblink WHERE type='cif:blog:html' "
                   "AND journo_id=%d" % journo_id)
    if cursor.fetchall():
        ukmedia.DBUG2('  not updating weblink for %s\n' % name)
    else:
        ukmedia.DBUG2('  NEW weblink: %s: %s\n' % (name, blog_url))
        cursor.execute(
            "INSERT INTO journo_weblink"
                "(journo_id, url, source, description, type, approved)"
            "VALUES (%s, %s, %s, %s, 'cif:blog:html', false)",
            [journo_id, blog_url, CONTRIBUTORS_URL, '"Comment is free" blog'])
    ukmedia.DBUG2('\n')

if __name__=='__main__':
    if '--help' in sys.argv or '-h' in sys.argv:
        sys.exit('usage: comment-is-free\n'
                 'Reads the contributors A-Z page, updates database.')
    run()

