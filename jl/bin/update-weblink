import sys
import time
import textwrap
sys.path.insert(0, '../pylib/JL')
import DB
sys.path.insert(0, '../scraper')
import wikipedia
import simplejson
from datetime import datetime
from wikipedia import NoSuchArticle


PARANOID = False  # tries JohnDoe_(journalist), etc. before JohnDoe

_conn = None  # DB connection, but use new_cursor() instead.
_reasons = {}

def new_cursor(msg):
    global _conn, _reasons
    if _conn is None:
        _conn = DB.Connect()
    #print 'new_cursor: %s: %s' % (msg, 'BEGIN')
    cur = _conn.cursor()
    cur.execute('BEGIN')
    _reasons[cur] = msg
    return cur

def end_cursor(cur, ok):
    if ok:
        command = 'COMMIT'
    else:
        command = 'ROLLBACK'
    #print 'end_cursor: %s: %s' % (_reasons[cur], command)
    cur.execute(command)
    cur.close()

def store_context(context, journo_ref, cursor):
    '''
    JSON-encodes context and stores it in scraped_wikipedia_journo table.
    JSON has no datetime support, so datetimes in context get stringized.
    '''
    bio = context['bio'].encode('utf-8')
    for k, v in context.iteritems():
        if isinstance(v, datetime):
            context[k] = v.isoformat()[:19]
    context_json = simplejson.dumps(context).encode('utf-8')
    cursor.execute("SELECT journo_ref FROM scraped_wikipedia_journo "
                   "WHERE journo_ref=%s", [journo_ref])
    if cursor.fetchone():
        cursor.execute("UPDATE scraped_wikipedia_journo SET context=%s, bio=%s "
                       "WHERE journo_ref=%s", [context_json, bio, journo_ref])
        cursor.fetchall()
    else:
        cursor.execute("INSERT INTO scraped_wikipedia_journo (journo_ref, context, bio) "
                       "VALUES (%s, %s, %s)", [journo_ref, context_json, bio])
        cursor.fetchall()

def scrape(journo_id, journo_ref, prettyname, url):
    print 'Trying:', url, '...'
    try:
        time.sleep(1)
        scraped = wikipedia.ScrapeArticle(url)
    except NoSuchArticle:
        pass
    except Exception, e:
        print 'Error:', e
    else:
        desc = 'Wikipedia: ' + prettyname
        c = new_cursor('scrape')
        try:
            c.execute("SELECT id FROM journo_weblink WHERE journo_id=%s", [journo_id])
            rows = c.fetchall()
            bio = store_context(scraped, journo_ref, c)
            source = 'scraped_wikipedia_journo'
            print 'Stored as %s:%s' % (source, journo_ref)
            print '\n    %s\n' % textwrap.fill(scraped['bio'].encode('utf-8')).replace('\n', '\n    ')
            if rows:
                assert len(rows)==1, 'Multiple weblinks found for journo %d!' % journo_id
                query = (
                    "UPDATE journo_weblink "
                    "SET url=%s, description=%s, source=%s "
                    "WHERE journo_id=%s")
                args = [url, desc, source, journo_id]
            else:
                query = (
                    "INSERT INTO journo_weblink (journo_id, url, description, source) "
                    "VALUES (%s, %s, %s, %s)")
                args = [journo_id, url, desc, source]
            c.execute(query, args)
            end_cursor(c, True)
        except:
            end_cursor(c, False)
            raise

def FindJournosNeedingWeblink():
    '''Returns a list of (rowid, ref, prettyname) pairs from the journo table.'''
    c = new_cursor('FindJournosNeedingWeblink')
    try:
        c.execute("SELECT id, ref, prettyname FROM journo "
                  "ORDER BY journo.id IN (SELECT journo_id FROM journo_weblink), prettyname")
        rows = c.fetchall()
    except:
        end_cursor(c, False)
        raise
    else:
        end_cursor(c, True)
        return rows

def FindJournoByRef(journo_ref):
    '''Returns a (rowid, ref, prettyname) pair from the journo table where ref=journo_ref.'''
    c = new_cursor('FindJournoByRef')
    try:
        c.execute("SELECT id, ref, prettyname FROM journo WHERE ref=%s", [journo_ref])
        row = c.fetchone()
        end_cursor(c, True)
        return row
    except:
        end_cursor(c, False)
        raise

def UpdateJournos(rows):
    for (journo_id, journo_ref, prettyname) in rows:
        wikiname = prettyname.replace(' ', '_')
        url = 'http://en.wikipedia.org/wiki/' + wikiname
        urls = [url]
        if PARANOID:
            urls = [
                url + '_(journalist)',
                url + '_(radio)',
                url + '_(tv)',
                url + '_(critic)',
                url + '_(commentator)',
                url + '_(author)',
                url
            ]
        for url in urls:
            scrape(journo_id, journo_ref, prettyname, url)

if __name__=='__main__':
    args = sys.argv[1:]
    usage = ('Scrapes Wikipedia, populates the scraped_wikipedia_journo table.\n'
             'usage: update-weblink [journo-ref]')
    if '--help' in args or '-h' in args:
        sys.exit(usage)
    if len(args)==0:
        rows = FindJournosNeedingWeblink()
    elif len(args)==1:
        row = FindJournoByRef(args[0])
        if row is None:
            sys.exit('error: no such journalist')
        rows = [row]
    else:
        sys.exit(usage)
    UpdateJournos(rows)
