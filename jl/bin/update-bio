import sys
import time
import textwrap
sys.path.insert(0, '../pylib/JL')
import DB
sys.path.insert(0, '../scraper')
import wikipedia
import simplejson
from datetime import datetime
from wikipedia import NoSuchArticle


PARANOID = False  # tries JohnDoe_(journalist), etc. before JohnDoe
QUIET = False

_conn = None  # DB connection, but use new_cursor() instead.
_reasons = {}

def new_cursor(msg):
    global _conn, _reasons
    if _conn is None:
        _conn = DB.Connect()
    #print 'new_cursor: %s: %s' % (msg, 'BEGIN')
    cur = _conn.cursor()
    cur.execute('BEGIN')
    _reasons[cur] = msg
    return cur

def end_cursor(cur, ok):
    if ok:
        command = 'COMMIT'
    else:
        command = 'ROLLBACK'
    #print 'end_cursor: %s: %s' % (_reasons[cur], command)
    cur.execute(command)
    cur.close()

def store_context(context, journo_id, cursor):
    '''
    JSON-encodes context and stores it in journo_bio table.
    JSON has no datetime support, so datetimes in context get stringized.
    '''
    bio = context['bio'].encode('utf-8')
    for k, v in context.iteritems():
        if isinstance(v, datetime):
            context[k] = v.isoformat()[:19]
    context_json = simplejson.dumps(context).encode('utf-8')
    cursor.execute("SELECT id FROM journo_bio "
                   "WHERE journo_id=%s LIMIT 2", [journo_id])
    rows = cursor.fetchall()
    assert len(rows)<2, "Multiple bios found, I don't know which to update"
    updated = False
    if rows:
        cursor.execute("UPDATE journo_bio SET context=%s, bio=%s, approved=false "
                       "WHERE journo_id=%s ",
                       [context_json, bio, journo_id])
        updated = True
        cursor.fetchall()
    else:
        cursor.execute("INSERT INTO journo_bio (journo_id, context, bio, approved) "
                       "VALUES (%s, %s, %s, false)", [journo_id, context_json, bio])
        cursor.fetchall()
    return bio, updated

def scrape(journo_id, journo_ref, prettyname, url):
    if not QUIET: print 'Trying:', url, '...'
    try:
        time.sleep(1)
        scraped = wikipedia.ScrapeArticle(url)
    except NoSuchArticle:
        pass
    except Exception, e:
        if not QUIET: print 'Error:', e
    else:
        desc = 'Wikipedia: ' + prettyname
        c = new_cursor('scrape')
        try:
            c.execute("SELECT id FROM journo_weblink WHERE journo_id=%s", [journo_id])
            rows = c.fetchall()
            bio, updated = store_context(scraped, journo_id, c)
            source = 'journo_bio'
            if not QUIET:
                action = ['Stored in', 'Updated'][updated]
                print '%s %s for journo %d (%s)' % (action, source, journo_id, journo_ref)
                print '\n    %s\n' % textwrap.fill(bio).replace('\n', '\n    ')
            if rows:
                assert len(rows)==1, 'Multiple weblinks found for journo %d!' % journo_id
                query = (
                    "UPDATE journo_weblink "
                    "SET url=%s, description=%s, source=%s "
                    "WHERE journo_id=%s")
                args = [url, desc, source, journo_id]
            else:
                query = (
                    "INSERT INTO journo_weblink (journo_id, url, description, source) "
                    "VALUES (%s, %s, %s, %s)")
                args = [journo_id, url, desc, source]
            c.execute(query, args)
            end_cursor(c, True)
        except:
            end_cursor(c, False)
            raise

def FindJournosNeedingWeblink():
    '''Returns a list of (rowid, ref, prettyname) pairs from the journo table.'''
    c = new_cursor('FindJournosNeedingWeblink')
    try:
        c.execute("SELECT id, ref, prettyname FROM journo "
                  "ORDER BY journo.status!='a',"  # active first, and prefer wikipedia-less
                  "         journo.id IN (SELECT journo_id FROM journo_weblink),"
                  "         prettyname")
        rows = c.fetchall()
    except:
        end_cursor(c, False)
        raise
    else:
        end_cursor(c, True)
        return rows

def FindJournoByRef(journo_ref):
    '''Returns a (rowid, ref, prettyname) pair from the journo table where ref=journo_ref.'''
    c = new_cursor('FindJournoByRef')
    try:
        c.execute("SELECT id, ref, prettyname FROM journo WHERE ref=%s", [journo_ref])
        row = c.fetchone()
        end_cursor(c, True)
        return row
    except:
        end_cursor(c, False)
        raise

def UpdateJournos(rows):
    for (journo_id, journo_ref, prettyname) in rows:
        wikiname = prettyname.replace(' ', '_')
        url = 'http://en.wikipedia.org/wiki/' + wikiname
        urls = [url]
        if PARANOID:
            urls = [
                url + '_(journalist)',
                url + '_(radio)',
                url + '_(tv)',
                url + '_(critic)',
                url + '_(commentator)',
                url + '_(author)',
                url
            ]
        for url in urls:
            scrape(journo_id, journo_ref, prettyname, url)

if __name__=='__main__':
    args = sys.argv[1:]
    usage = ('Scrapes Wikipedia, populates the journo_bio table.\n'
             'usage: update-bio [-q] [journo-ref]')
    if '--quiet' in args or '-q' in args:
        QUIET = True
        if '-q' in args: args.remove('-q')
        if '--quiet' in args: args.remove('--quiet')
    if '--help' in args or '-h' in args:
        sys.exit(usage)
    if len(args)==0:
        rows = FindJournosNeedingWeblink()
    elif len(args)==1:
        row = FindJournoByRef(args[0])
        if row is None:
            sys.exit('error: no such journalist')
        rows = [row]
    else:
        sys.exit(usage)
    UpdateJournos(rows)
