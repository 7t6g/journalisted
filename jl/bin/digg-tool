#!/usr/bin/env python2.4
# 2008-03-19  BenC  Initial version
#
# Scraper which looks for references to newspaper articles
# on digg.com and loads the number of diggs, comments etc
# into our database, populating the article_commentlink table.
#

import sys
from datetime import datetime
from optparse import OptionParser

sys.path.append( "../pylib" )
from digg import *
from JL import DB,ukmedia,CommentLink

# scraperfront used to map urls to article srcids
sys.path.append( "../scraper" )
import scrapefront


APPKEY = 'http://www.scumways.com'

domains = [
	'independent.co.uk',
	'dailymail.co.uk',
	'mailonsunday.co.uk',
	'express.co.uk',
	'dailyexpress.co.uk',
	'guardian.co.uk',
	'mirror.co.uk',
	'sundaymirror.co.uk',
	'telegraph.co.uk',
	'scotsman.com',
	'ft.com',
	'theherald.co.uk',
	'thesun.co.uk',
	'timesonline.co.uk',
	'bbc.co.uk'
]


digg = Digg(APPKEY)


def FetchFromDigg( domain, total=500 ):
	"""Try and find 'numentries' stories on Digg with the given domain"""
	entries = []
	got = 0
	while got < total:
		count = total-got
		if count > 100:
			count = 100

		errcnt = 0
		while 1:
			try:
				stories = digg.getStories( offset=got,count=count, domain=domain )
				break
			except Exception,err:
				if isinstance( err, KeyboardInterrupt ):
					raise
				errcnt += 1
				if errcnt >= 3:
					ukmedia.DBUG( "digg-tool: ABORTING - too many errors\n" )
					raise
				print >>sys.stderr, sys.exc_info()
				ukmedia.DBUG( "digg-tool: Retrying... (%d)\n" % (errcnt) )

		if total > int(stories.total):
			total = int(stories.total)

		count = int( stories.count )

		got += count
		ukmedia.DBUG2( "digg-tool: %s: got %d/%d\n" % (domain,got,total) )
		for s in stories:
			e = {
				'url': s.link,
				'score': s.diggs,
				'num_comments': s.comments,
				'comment_url': s.href,
				'source': 'digg',
#				'submitted': datetime.fromtimestamp( int( s.submit_date ) ),
				}
			entries.append(e)
	return entries


def LoadEntries( conn, entries ):
	"""Load fetched digg entries into the database"""

	stats = CommentLink.Stats()
	c = conn.cursor()
	for e in entries:
		srcid = scrapefront.CalcSrcID( e['url'] )
		if not srcid:
			# not handled
			stats.not_handled += 1
			continue
		e['srcid'] = srcid

		if CommentLink.AddCommentLink( conn, e ):
			stats.matched += 1
		else:
			stats.missing += 1

	return stats


def DoDomain( conn, domain ):
	"""Fetch digg entries for domain and try to load them into db"""
	entries = FetchFromDigg( domain )
	stats = LoadEntries( conn, entries )

	ukmedia.DBUG( "digg-tool: %s: %s\n" %( domain,stats.Report() ) )
	return stats



def main():
	conn = DB.Connect()

	overallstats = CommentLink.Stats()
	for d in domains:
		stats = DoDomain( conn, d )
		overallstats.Accumulate( stats )

	ukmedia.DBUG( "digg-tool: overall: %s" % (overallstats.Report()) )


if __name__ == "__main__":
	main()

