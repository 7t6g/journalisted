#!/usr/bin/env python
#
# indexer
#
# Tool to maintain a full-text index of the journalisted database
# using xapian.
#
#
# TODO:
# - add stopwords to indexer?
# - indexer should remove docs from xapian if status<>'a' and
#   needs_indexing='t' in postgres db. (and should have db triggers or
#   php code which marks modified articles for reindexing!)



# the "data" of each document in the xapian db is a json format object
# holding some of the article information. The field names are
# abbreviated to save space:
#
#  i: id
#  t: title
#  o: srcorg  (org id)
#  l: permalink
#  d: description
#  j: journo list 0 or more items consisting of:
#     i: journo_id, r: ref, n: prettyname

# ids for value fields we store along with the document
# We need the pubdate for restricting query by range, and
# the datetime version for sorting. xapian _should_ be able to use
# a datetime for range queries, but it can't yet... so date only.
XAP_PUBDATETIME_ID = 0      # "YYYYMMDDHHMMSS"
XAP_PUBDATE_ID = 1          # "YYYYMMDD"



import sys
import xapian
import string
from datetime import datetime
from optparse import OptionParser
try:
    import simplejson as json
except ImportError:
    import json

import site
site.addsitedir("../pylib")
from JL import DB,ukmedia

import mysociety.config
mysociety.config.set_file("../conf/general")
xapdbpath = mysociety.config.get('JL_XAPDB')


COMMIT_BATCH_SIZE=10000


# STOPWORDS (not used at the moment...)
stopwords = (
	'the',
	'a',
	'to',
	'and',
	'in',
	'of',
	'for',
	'on',
	'is',
	'with',
	'at',
	'that',
	'it',
	'as',
	'was',
	'by',
	'be',
	'from',
	'have',
	'but',
	'has',
	'an',
	'are',
	'this',
	'said',
	'not',
	'he',
	'been',
	'who',
	'will',
	'they',
	'which',
	'up',
	'had',
	'his',
	'their',
	'one',
	'after',
	'more',
	'out',
	'were',
	'when',
	'year',
	'all',
	'would',
	'there',
	'about',
	'we',
	'last',
	'or',
	'also',
	'than',
	'if',
	'two',
	'i',
	'over',
	'time',
	'so',
	'into',
	'first',
	'new',
	'no',
	'can',
	'now',
	'could',
	'its',
	'some',
	'what',
	'only',
	'just',
)


def IndexArticle( xapdb, art ):
    indexer = xapian.TermGenerator()

    stemmer = xapian.Stem("english")
    indexer.set_stemmer(stemmer)

#    stopper = xapian.SimpleStopper()
#    for s in stopwords:
#        stopper.add( s )
#    indexer.set_stopper(stopper)

    #print "indexing: '%s'"  %(art['title'].encode('utf-8'))

    # strip off html to get raw text
    txt = ukmedia.FromHTML( art['content'] );

    doc = xapian.Document()
#    doc.set_data(txt)

    # add article id as a term
    article_id_term = 'Q' + str(art['id'])
    doc.add_term( article_id_term )

    # add source org id as a term
    srcorg_id_term = 'O' + str(art['srcorg'])
    doc.add_term( srcorg_id_term )

    # add journo ids as terms to support search-by-journo
    for j in art['journos']:
        doc.add_term( 'J'+str(j['id']) )

    d = art['pubdate']
    doc.add_value( XAP_PUBDATETIME_ID, "%04d%02d%02d%02d%02d%02d" % ( d.year, d.month, d.day, d.hour, d.minute, d.second ) )
    doc.add_value( XAP_PUBDATE_ID, "%04d%02d%02d" % ( d.year, d.month, d.day ) )

    # add all the things we need to be able to display in search results...
    # serialise them and store them as the document data
    dat = {}
    dat['i']=art['id']
    dat['t']=art['title']
    dat['o']=art['srcorg']
    dat['l']=art['permalink']
    dat['d']=art['description']
    journo_list = []
    for j in art['journos']:
        journo_list.append( {'i':j['id'], 'r':j['ref'], 'n':j['prettyname']} )
    dat['j']=journo_list

    dat_serialised = json.dumps( dat, separators=(',',':') )

    #print dat_serialised

    doc.set_data( dat_serialised )

    # index the main text of the article...
    indexer.set_document(doc)
    indexer.index_text( txt )

    # ...and the title...
    indexer.increase_termpos()
    indexer.index_text( art['title'], 1, 'T' )

    # ...and authors
    for j in art['journos']:
        indexer.increase_termpos()
        indexer.index_text( j['prettyname'], 1, 'A' )

    xapdb.replace_document( article_id_term, doc )



def FetchJournos( conn, article_id ):
    c = conn.cursor()
    c.execute( """
        SELECT *
            FROM ( journo_attr attr INNER JOIN journo j ON j.id=attr.journo_id )
            WHERE attr.article_id=%s
        """, (article_id) )
    rows = c.fetchall()
    journos = []
    if rows is not None:
        for row in rows:
            journos.append( {
                'id':row['id'],
                'ref':row['ref'],
                'prettyname': row['prettyname'].decode('utf-8'),
                } )
    c.close()

    return journos


def PerformIndexing( xapdb,  options ):
    start = datetime.now()

    conn = DB.Connect()
    c = conn.cursor()

    if options.single_article_id is not None:
        # just force indexing of the one article
        article_id = int( options.single_article_id )
        ukmedia.DBUG2( "Indexing single article (id=%d).\n" % (article_id) )
        c.execute( "SELECT * FROM article WHERE id=%s", (article_id) )
    else:
        # retrieve articles
        sql = "SELECT * FROM article INNER JOIN article_needs_indexing ON id=article_id ORDER BY pubdate DESC"
        if options.max_number:
            sql = sql + " LIMIT %d" % ( int(options.max_number) )
        ukmedia.DBUG2( "fetching articles\n" )
        #print sql
        c.execute( sql );

    ukmedia.DBUG2( "indexing begins\n" )

    xapdb.begin_transaction()
    idx_cnt=0
    batch_cnt=0
    zap_cnt=0
    indexed_ids = []
    while 1:
        row = c.fetchone()
        if not row:
            break

        try:
            art = {}
            # decode all the fields that need to be in unicode
            for f in ( 'title', 'content', 'byline', 'description' ):
                art[f] = row[f].decode( 'utf-8' )
            # ones we can just copy without any processing
            for f in ( 'id','status','srcorg', 'pubdate','permalink' ):
                art[f] = row[f]

            if art['status'] == 'a':
                art['journos'] = FetchJournos( conn, art['id'] )
                IndexArticle( xapdb, art )
                idx_cnt = idx_cnt+1
                ukmedia.DBUG2( u"index '%s' [%d]\n" %(art['title'], art['id']) );
            else:
                # if article is not active, make sure it's removed from xapian
                Zap( xapdb, art['id'] )
                zap_cnt = zap_cnt + 1
                ukmedia.DBUG2( "zap   '%s' [%d]\n" %(art['title'].encode('utf-8'), art['id'] ) );

            indexed_ids.append( art['id'] )
        except Exception,e:
            sys.stderr.write('ERROR: failed on article %d - %s\n' % (row['id'],str(e)) )
            raise


        batch_cnt = batch_cnt+1
        if batch_cnt >= COMMIT_BATCH_SIZE:
            batch_cnt=0
            ukmedia.DBUG2( "committing %d...\n" %(batch_cnt) )
            xapdb.commit_transaction()
            FlagIndexed( conn, indexed_ids )
            conn.commit()
            indexed_ids = []
            xapdb.begin_transaction()

    # commit any leftovers
    if batch_cnt > 0:
        ukmedia.DBUG2( "committing %d...\n" %(batch_cnt) )
        xapdb.commit_transaction()
        FlagIndexed( conn, indexed_ids )
        conn.commit()
        indexed_ids = []
    ukmedia.DBUG( "done. indexed %d, removed %d\n" %(idx_cnt,zap_cnt) )

    c.close()

    fin = datetime.now()
    ukmedia.DBUG2( "finished. took %s\n" % (fin-start) )


def FlagIndexed( conn, article_ids ):
    """mark articles in the db as having been indexed"""
    c = conn.cursor()
    params = ['%s'] * len( article_ids )
    sql = "DELETE FROM article_needs_indexing WHERE article_id IN ( %s )" % ( ','.join(params) )
    c.execute( sql, article_ids )
    c.close()


def Zap( xapdb, article_id ):
    """ remove an article from the xapian DB """
    article_id_term = 'Q' + str(article_id)
    xapdb.delete_document( article_id_term )

def main():
    parser = OptionParser()

    parser.add_option("-z", "--zap",
        dest="zap_id",
        metavar="ARTICLE_ID",
        help="remove the entry for ARTICLE_ID" )
    parser.add_option("-a", "--articleid",
        dest="single_article_id",
        metavar="ARTICLE_ID",
        help="reindex the entry for ARTICLE_ID" )
#    parser.add_option("-r", "--replace",
#        action="store_true", dest="replace_existing",
#        help="reindex articles already in the xapian db")
    parser.add_option("-n", "--max-number",
        dest="max_number",
        help="Don't index any more articles than this (most recently-scraped first)")

    (options, args) = parser.parse_args()

    xapdb = xapian.WritableDatabase(xapdbpath, xapian.DB_CREATE_OR_OPEN)

    if options.zap_id:
        Zap( xapdb, options.zap_id )
    else:
        PerformIndexing( xapdb, options )

if __name__ == "__main__":
    main()

