#!/usr/bin/env python2.4
#
# indexer
#
# Tool to maintain a full-text index of the journalisted database
# using xapian.
#
#

import sys
import xapian
import string
from datetime import datetime
from optparse import OptionParser

import site
site.addsitedir("../pylib")
from JL import DB,ukmedia

import mysociety.config
mysociety.config.set_file("../conf/general")
xapdbpath = mysociety.config.get('JL_XAPDB')



# ids for value field we want to store along with the document
# (so we can display them in the search results)
# NOTE: these values need to be kept in sync with web/search.php!
XAP_ARTICLE_ID = 0
XAP_TITLE_ID = 1
XAP_PUBDATE_ID = 2
XAP_SRCORG_ID = 3
XAP_PERMALINK_ID = 4
XAP_JOURNOS_ID = 5


def IndexArticle( xapdb, art ):
    indexer = xapian.TermGenerator()
    stemmer = xapian.Stem("english")
    indexer.set_stemmer(stemmer)

    #print "indexing: '%s'"  %(art['title'].encode('utf-8'))
    txt = ukmedia.FromHTML( art['content'] );

    doc = xapian.Document()
#    doc.set_data(txt)

    # add article id as a term
    article_id_term = 'Q' + str(art['id'])
    doc.add_term( article_id_term )

    # TODO:
    # add source org as a term
    # - add name and srcorg...


    journo_list = []
    for j in art['journos']:
        # pretty name is already encoded as utf-8
        journo_list.append( "%s|%s" %(j['ref'],j['prettyname']) )
        # add journo ids as terms to support search-by-journo
        doc.add_term( 'J'+str(j['id']) )

    # add all the things we need to be able to display in search results...
    doc.add_value( XAP_ARTICLE_ID, str(art['id']) )
    doc.add_value( XAP_TITLE_ID, art['title'].encode('utf-8') )
    doc.add_value( XAP_PUBDATE_ID, art['pubdate'].isoformat() )
    doc.add_value( XAP_SRCORG_ID, str(art['srcorg']) )
    doc.add_value( XAP_PERMALINK_ID, art['permalink'] )
    # comma-separated list of attributed journo refs & names
    # eg: "bob-smith|Bob Smith,fred-blogs-1|Fred Blogs"
    doc.add_value( XAP_JOURNOS_ID, ','.join( journo_list ) )

    # index the main text of the article...
    indexer.set_document(doc)
    indexer.index_text( txt )

    # ...and the title...
    indexer.increase_termpos()
    indexer.index_text( art['title'], 1, 'T' )

    # ...and the byline.
    indexer.increase_termpos()
    indexer.index_text( art['byline'], 1, 'B' )


    xapdb.replace_document( article_id_term, doc )



def FetchJournos( conn, article_id ):
    c = conn.cursor()
    c.execute( """
        SELECT *
            FROM ( journo_attr attr INNER JOIN journo j ON j.id=attr.journo_id )
            WHERE attr.article_id=%s
        """, (article_id) )
    journos = c.fetchall()
    if journos == None:
        journos = []
    c.close()
    return journos


def PerformIndexing( options ):
    start = datetime.now()

    conn = DB.Connect()
    xapdb = xapian.WritableDatabase(xapdbpath, xapian.DB_CREATE_OR_OPEN)

    conditions = [ "status=%s" ]
    params = [ 'a' ]
    if options.from_date:
        conditions.append( "lastscraped >= %s::date" )
        params.append( options.from_date )

    if options.to_date:
        # +1 day to include the day of to_date
        conditions.append( "lastscraped < %s::date + '1 day'::interval" )
        params.append( options.to_date )

    print "counting..."
    sql = "SELECT count(*) FROM article WHERE " + " AND ".join( conditions )
    c = conn.cursor()
    c.execute( sql, params );
    expected = c.fetchone()[0]
    print "%d articles to process." % (expected)

    sql = "SELECT * FROM article WHERE " + " AND ".join( conditions )
    print "fetching articles"

    c.execute( sql, params );

    print "indexing begins"

    xapdb.begin_transaction()
    tot=0
    cnt=0
    skipped=0
    while 1:
        row = c.fetchone()
        if not row:
            break
        art = {}
        # decode all the fields that need to be in unicode
        for f in ( 'title', 'content', 'byline' ):
            art[f] = row[f].decode( 'utf-8' )
        # plain ascii ones...
        for f in ( 'id','srcorg', 'pubdate','permalink' ):
            art[f] = row[f]

        skip = False
        if not options.replace_existing:
            skip = xapdb.term_exists( 'Q' + str(art['id']) )

        if not skip:
            art['journos'] = FetchJournos( conn, art['id'] )
            IndexArticle( xapdb, art )
# TODO - set a flag in article table to indicate article has been indexed!!!
#            updatecursor.execute( "UPDATE article SET needs_indexing=false" WHERE id=%s" , (art['id'] )

            cnt = cnt+1
            tot = tot+1
            if cnt >= 1000:
                cnt=0
                print "committing..."
                xapdb.commit_transaction()
                print "=> committed %d" %(tot)
                xapdb.begin_transaction()
        else:
            skipped = skipped+1


    # commit any leftovers
    xapdb.commit_transaction()
    print "done. committed %d, skipped %d" %(tot,skipped)

    c.close()

    fin = datetime.now()
    print "finished. took %s " % (fin-start)


def main():
    parser = OptionParser()

    parser.add_option("-f", "--from-date",
        dest="from_date",
        metavar="DATE",
        help="index articles scraped from DATE (yyyy-mm-dd) onward" )
    parser.add_option("-t", "--to-date",
        dest="to_date",
        metavar="DATE",
        help="index articles scraped up to (and including) DATE (yyyy-mm-dd)" )
    parser.add_option("-r", "--replace",
        action="store_true", dest="replace_existing",
        help="reindex articles already in the xapian db")

    (options, args) = parser.parse_args()

    PerformIndexing( options )



if __name__ == "__main__":
    main()

