#!/usr/bin/env python2.4
#
# indexer
#
# Tool to maintain a full-text index of the journalisted database
# using xapian.
#
#

import sys
import xapian
import string
from datetime import datetime
from optparse import OptionParser

import site
site.addsitedir("../pylib")
from JL import DB,ukmedia

import mysociety.config
mysociety.config.set_file("../conf/general")
xapdbpath = mysociety.config.get('JL_XAPDB')


COMMIT_BATCH_SIZE=1000


# ids for value field we want to store along with the document
# (so we can display them in the search results)
# NOTE: these values need to be kept in sync with web/search.php!
XAP_ARTICLE_ID = 0
XAP_TITLE_ID = 1
XAP_PUBDATE_ID = 2
XAP_SRCORG_ID = 3
XAP_PERMALINK_ID = 4
XAP_JOURNOS_ID = 5


def IndexArticle( xapdb, art ):
    indexer = xapian.TermGenerator()
    stemmer = xapian.Stem("english")
    indexer.set_stemmer(stemmer)

    #print "indexing: '%s'"  %(art['title'].encode('utf-8'))
    txt = ukmedia.FromHTML( art['content'] );

    doc = xapian.Document()
#    doc.set_data(txt)

    # add article id as a term
    article_id_term = 'Q' + str(art['id'])
    doc.add_term( article_id_term )

    # TODO:
    # add source org as a term
    # - add name and srcorg...


    journo_list = []
    for j in art['journos']:
        # pretty name is already encoded as utf-8
        journo_list.append( "%s|%s" %(j['ref'],j['prettyname']) )
        # add journo ids as terms to support search-by-journo
        doc.add_term( 'J'+str(j['id']) )

    # add all the things we need to be able to display in search results...
    doc.add_value( XAP_ARTICLE_ID, str(art['id']) )
    doc.add_value( XAP_TITLE_ID, art['title'].encode('utf-8') )
    doc.add_value( XAP_PUBDATE_ID, art['pubdate'].isoformat() )
    doc.add_value( XAP_SRCORG_ID, str(art['srcorg']) )
    doc.add_value( XAP_PERMALINK_ID, art['permalink'] )
    # comma-separated list of attributed journo refs & names
    # eg: "bob-smith|Bob Smith,fred-blogs-1|Fred Blogs"
    doc.add_value( XAP_JOURNOS_ID, ','.join( journo_list ) )

    # index the main text of the article...
    indexer.set_document(doc)
    indexer.index_text( txt )

    # ...and the title...
    indexer.increase_termpos()
    indexer.index_text( art['title'], 1, 'T' )

    # ...and the byline.
    indexer.increase_termpos()
    indexer.index_text( art['byline'], 1, 'B' )


    xapdb.replace_document( article_id_term, doc )



def FetchJournos( conn, article_id ):
    c = conn.cursor()
    c.execute( """
        SELECT *
            FROM ( journo_attr attr INNER JOIN journo j ON j.id=attr.journo_id )
            WHERE attr.article_id=%s
        """, (article_id) )
    journos = c.fetchall()
    if journos == None:
        journos = []
    c.close()
    return journos


def PerformIndexing( xapdb,  options ):
    start = datetime.now()

    conn = DB.Connect()

    conditions = [ "status=%s" ]
    params = [ 'a' ]

    conditions.append( "needs_indexing=%s" )
    params.append( 'true' )

    if options.from_date:
        conditions.append( "lastscraped >= %s::date" )
        params.append( options.from_date )

    if options.to_date:
        # +1 day to include the day of to_date
        conditions.append( "lastscraped < %s::date + '1 day'::interval" )
        params.append( options.to_date )

    ukmedia.DBUG2( "counting...\n" )
    sql = "SELECT count(*) FROM article WHERE " + " AND ".join( conditions )
    if options.max_number:
        sql = sql + " LIMIT %d" % ( int(options.max_number) )

    c = conn.cursor()


    c.execute( sql, params );
    expected = c.fetchone()[0]
    if options.max_number:
        if expected > int( options.max_number ):
            expected = int( options.max_number )
    ukmedia.DBUG2( "%d articles to process.\n" % (expected) )

    sql = "SELECT * FROM article WHERE " + " AND ".join( conditions ) + " ORDER BY lastscraped DESC"
    if options.max_number:
        sql = sql + " LIMIT %d" % ( int(options.max_number) )

    ukmedia.DBUG2( "fetching articles\n" )
    #print sql

    c.execute( sql, params );

    ukmedia.DBUG2( "indexing begins\n" )

    xapdb.begin_transaction()
    tot=0
    cnt=0
    skipped=0
    indexed_ids = []
    while 1:
        row = c.fetchone()
        if not row:
            break
        art = {}
        # decode all the fields that need to be in unicode
        for f in ( 'title', 'content', 'byline' ):
            art[f] = row[f].decode( 'utf-8' )
        # plain ascii ones...
        for f in ( 'id','srcorg', 'pubdate','permalink' ):
            art[f] = row[f]

        skip = False
#        if not options.replace_existing:
#            skip = xapdb.term_exists( 'Q' + str(art['id']) )

        if not skip:
            art['journos'] = FetchJournos( conn, art['id'] )
            IndexArticle( xapdb, art )
            indexed_ids.append( art['id'] )

            cnt = cnt+1
            tot = tot+1
            if cnt >= COMMIT_BATCH_SIZE:
                cnt=0
                ukmedia.DBUG2( "committing...\n" )
                xapdb.commit_transaction()
                FlagIndexed( conn, indexed_ids )
                indexed_ids = []
                ukmedia.DBUG2( "=> committed %d\n" %(tot) )
                xapdb.begin_transaction()
        else:
            skipped = skipped+1


    # commit any leftovers
    if cnt > 0:
        xapdb.commit_transaction()
        FlagIndexed( conn, indexed_ids )
        indexed_ids = []
    ukmedia.DBUG2( "done. committed %d, skipped %d\n" %(tot,skipped) )

    c.close()

    fin = datetime.now()
    ukmedia.DBUG2( "finished. took %s\n" % (fin-start) )


def FlagIndexed( conn, article_ids ):
    """mark articles in the db as having been indexed"""
    c = conn.cursor()
    params = ['%s'] * len( article_ids )
    sql = "UPDATE article SET needs_indexing=false WHERE id IN ( %s )" % ( ','.join(params) )
    c.execute( sql, article_ids )
    conn.commit()
    c.close()


def Zap( xapdb, article_id ):
    """ remove an article from the xapian DB """
    article_id_term = 'Q' + article_id
    xapdb.delete_document( article_id_term )

def main():
    parser = OptionParser()

    parser.add_option("-f", "--from-date",
        dest="from_date",
        metavar="DATE",
        help="index articles scraped from DATE (yyyy-mm-dd) onward" )
    parser.add_option("-t", "--to-date",
        dest="to_date",
        metavar="DATE",
        help="index articles scraped up to (and including) DATE (yyyy-mm-dd)" )
    parser.add_option("-z", "--zap",
        dest="zap_id",
        metavar="ARTICLE_ID",
        help="remove the entry for ARTICLE_ID" )
#    parser.add_option("-r", "--replace",
#        action="store_true", dest="replace_existing",
#        help="reindex articles already in the xapian db")
    parser.add_option("-n", "--max-number",
        dest="max_number",
        help="Don't index any more articles than this (most recently-scraped first)")

    (options, args) = parser.parse_args()

    xapdb = xapian.WritableDatabase(xapdbpath, xapian.DB_CREATE_OR_OPEN)

    if options.zap_id:
        Zap( xapdb, options.zap_id )
    else:
        PerformIndexing( xapdb, options )

if __name__ == "__main__":
    main()

