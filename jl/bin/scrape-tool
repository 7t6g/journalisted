#!/usr/bin/env python2.4

import sys
from optparse import OptionParser

sys.path.append( "../pylib" )
from JL import ArticleDB,ukmedia,ScraperUtils

sys.path.append( "../scraper" )
import scrapefront



def main():
    parser = OptionParser()
    parser.add_option( "-u", "--url", dest="url", help="scrape a single article from URL", metavar="URL" )
    parser.add_option( "-d", "--dryrun", action="store_true", dest="dryrun", help="don't touch the database")
    parser.add_option( "-s", "--srcid", dest="srcid", action="store_true", help="just calculate the srcid for the given url (-u) and exit" )

    (options, args) = parser.parse_args()

    url = options.url

    if options.srcid:
        srcid = scrapefront.CalcSrcID( url )
        if srcid is None:
            return 1
        print srcid
        return 0

    scraper = scrapefront.PickScraper( url )
    if not scraper:
        sys.exit( "ERROR: No scraper found to handle url '%s'" % (url) )

    if options.dryrun:
        store = ArticleDB.ArticleDB( dryrun=True, reallyverbose=True )
    else:
        store = ArticleDB.ArticleDB()

    articles = []
    context = scraper.ContextFromURL( url )
    articles.append( context )


    # call any scraper init required (eg Login)
    if hasattr( scraper, "Prep" ):
        scraper.Prep()

    ScraperUtils.ProcessArticles( articles, store, scraper.Extract,
        extralogging=True)
    return 0

if __name__ == "__main__":
    sys.exit( main() )

