#!/usr/bin/env python2.4
#
# scraper to find blog posts which reference articles we cover, using icerocket.com
#
# note: icerocket has rss feeds for their search results, but there is more
# information in the normal html results - specifically the actual link to
# the article, which we need. So we'll do html scrapery stuff.
#

from optparse import OptionParser
import sys
import re
import traceback
import time
import urllib
import urlparse
from datetime import datetime,timedelta

sys.path.append( "../pylib" )
from JL import ukmedia,DB
from BeautifulSoup import BeautifulSoup

# scraperfront used to map urls to article srcids
sys.path.append( "../scraper" )
import scrapefront


HARD_MAX_PAGES = 20     # never query more than this per site


# TODO: should be in a config file
sitenames = (
    'independent.co.uk',
    'dailymail.co.uk',
    'express.co.uk',
    'dailyexpress.co.uk',
    'guardian.co.uk',
    'thesun.co.uk',
    'sundaymirror.co.uk',
    'mirror.co.uk',
    'telegraph.co.uk',
    'thescotsman.scotsman.com',
    'scotlandonsunday.scotsman.com',
    'ft.com',
    'theherald.co.uk',
    'timesonline.co.uk',
    'news.bbc.co.uk',
    'newsoftheworld.co.uk' )


_options = None



when_pat = re.compile( r'(\d+)\s+(second|minute|hour|day|month|year)s?\b' )

def ParseWhen( whentxt ):
    """turn a phrase of form '5 minutes ago', '1 day ago' etc... into datetime object"""

    m = when_pat.search( whentxt )
    num = int( m.group(1) )
    unit = m.group(2)

    now = datetime.now()
    if unit=='second':
        return now - timedelta( seconds=num )
    if unit=='minute':
        return now - timedelta( minutes=num )
    if unit=='hour':
        return now - timedelta( hours=num )
    if unit=='day':
        return now - timedelta( days=num )
    if unit=='month':
        return now - timedelta( months=num )
    if unit=='year':
        return now - timedelta( years=num )

    assert( 0 )
    return None


def ParseIcerocketResultsPage( html ):
    """Parse a results page from icerocket.com, returning a list of bloglinks"""
    soup = BeautifulSoup( html )


    bloglinks = []

#    latest_td = soup.find('td',{'id':'latest'} )
#    txt = ukmedia.FromHTMLOneLine( latest_td.renderContents( None ) )
#    print txt

    result_cnt = 0
    name_pat = re.compile( r"(.*) - .*?", re.DOTALL )
    for table in soup.findAll( 'table', {'class':'content'} ):

        b = { 'source': 'icerocket' }

        # title and permalink of the post
        main_link = table.find('a', {'class':'main_link'} )
        b['title'] = main_link.renderContents(None).strip()
        b['title'] = ukmedia.FromHTMLOneLine( b['title'] )
        b['nearestpermalink'] = main_link['href']


        # get the blog name and url
        #eg <a class="blogl" href="http://gbsteve.livejournal.com/">-- GBS -- - gbsteve.livejournal.com</a>
        blogl = table.find('a', {'class':'blogl'} )
        if blogl is None:
            # not always a link to the blog mainpage... derive from post url
            o = urlparse.urlparse( b['nearestpermalink'] )
            b['blogurl'] = unicode( o[1] )
            b['blogname'] = unicode( o[1] )
        else:
            b['blogurl'] = blogl['href']
            m = name_pat.match( blogl.renderContents(None) )
            b['blogname'] = ukmedia.FromHTMLOneLine( m.group(1) )

        # when was it posted? (also contains author name, but we don't use that)
        # eg "47 minutes ago", "1 day ago"
        when = table.find('p', {'class':'signed'} ).span.renderContents(None)
        b['linkcreated'] = ParseWhen( when )

        # get text excerpt (and it should contain the article link that we are looking for - hooray!)
        cut = table.find('p', {'class':'cut'} )
        b['excerpt'] = cut.renderContents(None).strip()
        b['excerpt'] = ukmedia.FromHTMLOneLine( b['excerpt'] )

#        print soup.originalEncoding, b['title'].__class__, b['title'].encode('utf-8')
        # look for links to news articles we cover

        for a in cut.findAll( 'a' ):
            url = a['href']
            srcid = scrapefront.CalcSrcID( url )
            if srcid is not None:
                # it's a link to an article we might have!
                bloglink = b.copy()
                bloglink['article_srcid'] = srcid
                bloglink['article_url'] = url
                bloglinks.append( bloglink )

        result_cnt = result_cnt + 1

    return (result_cnt, bloglinks)


# TODO: factor this out!
def LoadBlogLinkIntoDB( conn, l ):
    """ Try and load a single blog link entry into the article_bloglink table

    returns "nomatch", "alreadygot" or "added"
    """
    
    c = conn.cursor()

    assert ('article_srcid' in l) or ('article_url' in l)

    if 'article_srcid' not in l:
        srcid = scrapefront.CalcSrcID( l['article_url'] )
        if srcid == None:
            # url is not handled by our scrapers...
            return "nomatch"
        l['article_srcid'] = srcid

    # Do we have that article in our DB?
    c.execute( "SELECT id FROM article WHERE srcid=%s", l['article_srcid'] )
    articles = c.fetchall()
    if len(articles) < 1:
        # can't find article in DB
        return "nomatch"
    assert len(articles)==1
    article_id = articles[0]['id']

    # already got this bloglink?
    c.execute( "SELECT id FROM article_bloglink WHERE nearestpermalink=%s AND article_id=%s",
        l['nearestpermalink'], article_id );
    row = c.fetchone()
    if row:
        # already in db
        bloglinkid = row['id']
        return "alreadygot"

    # now insert it into the database
    c.execute( """INSERT INTO article_bloglink
        ( article_id, nearestpermalink, title, blogname, blogurl, linkcreated, excerpt, source )
        VALUES ( %s,%s,%s,%s,%s,%s,%s,%s )""",
        article_id,
        l['nearestpermalink'],
        l['title'].encode('utf-8'),
        l['blogname'].encode('utf-8'),
        l['blogurl'],
        "%s" %(l['linkcreated']),
        l['excerpt'].encode('utf-8'),
        l['source'] )

#    c.execute( "select currval('article_bloglink_id_seq')" )
#    bloglinkid = c.fetchone()[0]
#   print "new blog link (%s) to article '%s': '%s'" % ( bloglinkid, l['article_srcid'], l['nearestpermalink'] )
    c.close()

    return "added"





def DoSite( conn, s ):
    global _options

    oldest = datetime.now() - timedelta( hours=_options.max_age )

    num_per_page = 200  # max allowed by icerocket
    # need to be tolerant - sometimes there is borked data
    # (eg bad links in snippets from original blog posts)
    allowed_errs = 5   # per site
    errcnt = 0
    stop = False
    page =1 
    while( not stop and page < HARD_MAX_PAGES+1 ):
        try:
            params = urllib.urlencode( {
                'tab': 'blog',
                'q': 'link:%s'%(s),
                'n': num_per_page,
                'p': page } )

            url = 'http://www.icerocket.com/search?' + params
#            if _options.verbose:
#                print "fetching %s" % (url)
            html = ukmedia.FetchURL( url )
            (result_cnt,bloglinks) = ParseIcerocketResultsPage( html )


            cnts = {'added':0,'alreadygot':0,'nomatch':0}
            for b in bloglinks:
                if b['linkcreated']<oldest:
                    # got enough. stop after this page.
                    stop = True

                foo = LoadBlogLinkIntoDB( conn, b )
                cnts[foo] = cnts[foo] + 1
                if _options.dryrun:
                    conn.rollback()
                else:
                    conn.commit()

            if _options.verbose:
                print "%s (%d): got %d, from %d items => %d added, %d alreadygot, %d nomatch" % (
                    s, page, len(bloglinks), result_cnt, cnts['added'], cnts['alreadygot'], cnts['nomatch'] )
        except (Exception), e:
            # always just bail out upon ctrl-c
            if isinstance( e, KeyboardInterrupt ):
                raise
            msg = u"ERROR doing %s, page %d (%s)" % (s, page,e.__class__)
            print >>sys.stderr, msg.encode( 'utf-8' )
            print >>sys.stderr, '-'*60
            print >>sys.stderr, traceback.format_exc()
            print >>sys.stderr, '-'*60

            errcnt = errcnt + 1
            if errcnt > allowed_errs:
                print >>sys.stderr, "Too many errors - ABORTING %s" % (s)
                raise


        page=page+1

    
def main():
    global _options

    parser = OptionParser()

    parser.add_option("-v", "--verbose", action="store_true", dest="verbose", help="output progress information")
    parser.add_option( "-d", "--dryrun", action="store_true", dest="dryrun", help="don't touch the database")
    parser.add_option("-a", "--maxage",
        dest="max_age",
        type="int",
        default=24,
        help="keep fetching pages until we get links older than max_age hours old (default 24 )")
#    parser.add_option("-p", "--pages",
#        dest="num_pages",
#        type="int",
#        default=1,
#        help="how many icerocket query pages to try for per news site (default 1). ignored if -a set")
    parser.add_option("-s", "--site",
        action="store", dest="single_site",
        help="look for bloglinks to given site (eg 'dailymail.co.uk')")

    (_options, args) = parser.parse_args()

    conn = DB.Connect()

    if _options.single_site:
        DoSite( conn, _options.single_site )
    else:
        for s in sitenames:
            DoSite( conn, s )

if __name__ == "__main__":
    main()



