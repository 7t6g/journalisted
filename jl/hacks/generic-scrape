#!/usr/bin/env python
#
# tool to try and scrape some minimal info from _any_ news article page
# title, pubdate...
#

import sys
from datetime import datetime
from optparse import OptionParser
import simplejson as json
import re
import dateutil.parser

import site
site.addsitedir("../pylib")
from JL import DB,ukmedia
from BeautifulSoup import BeautifulSoup, Comment

_options = None



def find_story( soup ):
    # try and identify a container around just the story part
    # id/class markers, in order of preference
    story_markers = (
        re.compile( r'\bhentry\b|\bpost\b|\bstory\b', re.IGNORECASE ),
        re.compile( r'\bcontent\b|\bmain\b|\bcontentContainer\b|\bcontent[-_]main\b|\bmainColumn\b', re.IGNORECASE )
    )

    story = None
    for pat in story_markers:
        for attr in ('id','class'):
            story = soup.find( 'div', {attr:pat} )
            if story is not None:
                break   # got one!
        if story is not None:
            break   # got one!
    return story



def find_headline( soup ):

    headline =  None
    # try some easy cases first:
    h = soup.find( ('h1','h2','h3'),{'class':re.compile('entry-title|headline')}) 
    if h is not None:
        return h

    story = find_story( soup )
    if story is None:
        # give up and just use the whole page
        story = soup

    # might be one with .headline class...
    headline = story.find( ('h1','h2','h3'), {'class':re.compile('title|headline')})
    if headline is not None:
        return headline
    # just go for biggest h[123] we can find
    for h in ('h1','h2','h3'):
        headline = story.find(h)
        if headline is not None:
            return headline

    return None



def extract_pubdate( soup ):
    pubdate = None

    # look for hAtom/hNews dates...
    # <abbr class="foo" title="YYYY-MM-DDTHH:MM:SS+ZZ:ZZ">
    abbr = soup.find( 'abbr', {'class':'published'} )
    if abbr is None:
        abbr = soup.find( 'abbr', {'class':'updated'} )

    if abbr is not None:
        pubdate = dateutil.parser.parse( abbr['title'] )

    return pubdate
    # let's just leave it there for now.

    # NEVER GETS HERE


    # a bunch of things which might indicate a date/time...
    dt_markers = (
        'mon', 'tue', 'wed', 'thu' 'fri', 'sat', 'sun',
        'monday', 'tuesday','wednesday','thursday','friday','saturday','sunday',
        'jan','feb','mar','apr','may','jun',
        'jul','aug','sep','oct','nov','dec',
        'january', 'february', 'march', 'april', 'may', 'june',
        'july', 'august', 'september', 'october', 'november', 'december',
        r'2\d\d\d', r'1\d\d\d', # year
        r'\d\d:\d\d',           # time
        r'\d\d-\d\d',r'\d\d/\d\d'   # 01/02 or 01-02
    )

    t = '|'.join( [ r'\b'+marker+r'\b' for marker in dt_markers ] )
#    print t
    dt_pat = re.compile( t )

    pubdate = None
    for txt in soup.findAll( text=dt_pat ):
        try:
            rawtxt = str(txt).strip()
            pubdate = dateutil.parser.parse( txt, fuzzy=True )
            if pubdate is not None:
                break
        except Exception:
            pass

    return pubdate




def extract( html, context ):
    soup = BeautifulSoup( html )

    title = u''
    h = find_headline( soup )
    if h is not None:
        title = ukmedia.FromHTMLOneLine( h.renderContents(None) )
    #ideas:
    # check for a slug in the url and make sure the headline corresponds
    # ditto, using page <title>

    pubdate = extract_pubdate(soup)
#    if pubdate is None:
        # give up and just use the whole page
#        pubdate = extract_date(soup)

    context['title'] = title
    context['pubdate'] = pubdate
    return context


def main():
    global _options
    parser = OptionParser()
#    parser.add_option("-v", "--verbose", action="store_true", dest="verbose", help="output progress information")
#    parser.add_option("-j", "--json", action="store_true", dest="json", help="output results as json")

    (_options, args) = parser.parse_args()

    results = []
    for url in args:
        try:
            html = ukmedia.FetchURL( url )
            details = extract( html, {'url':url,'status':'ok'} )
        except Exception,err:
            details = {
                'url':url,
                'status':'error',
                'errormsg': str(err) }

        results.append( details )

    print( json.dumps( results ) )

if __name__ == "__main__":
    main()

